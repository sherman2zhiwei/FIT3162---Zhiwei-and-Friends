{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Testing - Unittest\n",
    "## Load python unittest module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Extraction\n",
    "### Functions that need to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabs(datasets):\n",
    "    \"\"\"Build vocabulary from an iterable of datasets objects\n",
    "\n",
    "    Args:\n",
    "        datasets: a list of dataset objects\n",
    "\n",
    "    Returns:\n",
    "        a set of all the words in the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab_words = set()\n",
    "    vocab_tags = set()\n",
    "    for dataset in datasets:\n",
    "        for words, tags in dataset:\n",
    "            #print(words)\n",
    "            vocab_words.update(words)\n",
    "            vocab_tags.update(tags)\n",
    "    print(\"- done. {} tokens\".format(len(vocab_words)))\n",
    "    return vocab_words, vocab_tags\n",
    "\n",
    "\n",
    "def get_char_vocab(dataset):\n",
    "    \"\"\"Build char vocabulary from an iterable of datasets objects\n",
    "\n",
    "    Args:\n",
    "        dataset: a iterator yielding tuples (sentence, tags)\n",
    "\n",
    "    Returns:\n",
    "        a set of all the characters in the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    vocab_char = set()\n",
    "    for words, _ in dataset:\n",
    "        for word in words:\n",
    "            vocab_char.update(word)\n",
    "\n",
    "    return vocab_char\n",
    "\n",
    "\n",
    "def get_glove_vocab(filename):\n",
    "    \"\"\"Load vocab from file\n",
    "\n",
    "    Args:\n",
    "        filename: path to the glove vectors\n",
    "\n",
    "    Returns:\n",
    "        vocab: set() of strings\n",
    "    \"\"\"\n",
    "    print(\"Building vocab Glove...\")\n",
    "    vocab = set()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split(' ')[0]\n",
    "            vocab.add(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def write_vocab(vocab, filename):\n",
    "    \"\"\"Writes a vocab to a file\n",
    "\n",
    "    Writes one word per line.\n",
    "\n",
    "    Args:\n",
    "        vocab: iterable that yields word\n",
    "        filename: path to vocab file\n",
    "\n",
    "    Returns:\n",
    "        write a word per line\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Writing vocab...\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if i != len(vocab) - 1:\n",
    "                f.write(\"{}\\n\".format(word))\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "\n",
    "\n",
    "def load_vocab(filename):\n",
    "    \"\"\"Loads vocab from a file\n",
    "\n",
    "    Args:\n",
    "        filename: (string) the format of the file must be one word per line.\n",
    "\n",
    "    Returns:\n",
    "        d: dict[word] = index\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = dict()\n",
    "        with open(filename) as f:\n",
    "            for idx, word in enumerate(f):\n",
    "                word = word.strip()\n",
    "                d[word] = idx\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "    return d\n",
    "\n",
    "\n",
    "def export_trimmed_glove_vectors(vocab, glove_filename, trimmed_filename, dim):\n",
    "    \"\"\"Saves glove vectors in numpy array\n",
    "\n",
    "    Args:\n",
    "        vocab: dictionary vocab[word] = index\n",
    "        glove_filename: a path to a glove file\n",
    "        trimmed_filename: a path where to store a matrix in npy\n",
    "        dim: (int) dimension of embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    with open(glove_filename) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if word in vocab:\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(embedding)\n",
    "\n",
    "    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
    "\n",
    "\n",
    "def get_trimmed_glove_vectors(filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filename: path to the npz file\n",
    "\n",
    "    Returns:\n",
    "        matrix of embeddings (np array)\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with np.load(filename) as data:\n",
    "            return data[\"embeddings\"]\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "\n",
    "\n",
    "def get_processing_word(vocab_words=None, vocab_chars=None,\n",
    "                    lowercase=False, chars=False, allow_unk=True):\n",
    "    \"\"\"Return lambda function that transform a word (string) into list,\n",
    "    or tuple of (list, id) of int corresponding to the ids of the word and\n",
    "    its corresponding characters.\n",
    "\n",
    "    Args:\n",
    "        vocab: dict[word] = idx\n",
    "\n",
    "    Returns:\n",
    "        f(\"cat\") = ([12, 4, 32], 12345)\n",
    "                 = (list of char ids, word id)\n",
    "\n",
    "    \"\"\"\n",
    "    def f(word):\n",
    "        # 0. get chars of words\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            char_ids = []\n",
    "            for char in word:\n",
    "                # ignore chars out of vocabulary\n",
    "                if char in vocab_chars:\n",
    "                    char_ids += [vocab_chars[char]]\n",
    "\n",
    "        # 1. preprocess word\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = NUM\n",
    "\n",
    "        # 2. get id of word\n",
    "        if vocab_words is not None:\n",
    "            if word in vocab_words:\n",
    "                word = vocab_words[word]\n",
    "            else:\n",
    "                if allow_unk:\n",
    "                    word = vocab_words[UNK]\n",
    "                else:\n",
    "                    raise Exception(\"Unknow key is not allowed. Check that \"\\\n",
    "                                    \"your vocab (tags?) is correct\")\n",
    "\n",
    "        # 3. return tuple char ids, word id\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            return char_ids, word\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        max_length = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    elif nlevels == 2:\n",
    "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                               for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                [pad_tok]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                max_length_sentence)\n",
    "    if nlevels ==1:\n",
    "        return sequence_padded, sequence_length, max_length\n",
    "    else:\n",
    "        return sequence_padded, sequence_length\n",
    "\n",
    "def minibatches(data, minibatch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: generator of (sentence, tags) tuples\n",
    "        minibatch_size: (int)\n",
    "\n",
    "    Yields:\n",
    "        list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x, y) in data:\n",
    "        if len(x_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags,message=None):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    default = tags[NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "    #print(message + \"{}{}{}\".format(seq, tags, chunks))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Aggregation\n",
    "### Functions that need to be tested\n",
    "The functions used for aspect aggregation are copied here because of some modification on them. The reason why we want to modify them is because some parts of the functions are not able to be tested (e.g. model prediction). Hence, copying functions to this file reduces the risk of having buggy code and also allows us to have more flexibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 13 02:23:50 2019\n",
    "\n",
    "@author: Zhiwei and Friend(s)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "###########################################\n",
    "  _      _ _                    _          \n",
    " | |    (_| |                  (_)         \n",
    " | |     _| |__  _ __ __ _ _ __ _  ___ ___ \n",
    " | |    | | '_ \\| '__/ _` | '__| |/ _ / __|\n",
    " | |____| | |_) | | | (_| | |  | |  __\\__ \\\n",
    " |______|_|_.__/|_|  \\__,_|_|  |_|\\___|___/\n",
    "                                           \n",
    "###########################################\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\"\"\"\n",
    "###############################################\n",
    "  ______                _   _                 \n",
    " |  ____|              | | (_)                \n",
    " | |__ _   _ _ __   ___| |_ _  ___  _ __  ___ \n",
    " |  __| | | | '_ \\ / __| __| |/ _ \\| '_ \\/ __|\n",
    " | |  | |_| | | | | (__| |_| | (_) | | | \\__ \\\n",
    " |_|   \\__,_|_| |_|\\___|\\__|_|\\___/|_| |_|___/\n",
    "\n",
    "###############################################\n",
    "\"\"\"\n",
    "\n",
    "# Flatten list of list\n",
    "def _flatten(l):\n",
    "    \"\"\"\n",
    "    This function will flatten a list of list to a list. (e.g. [[1],[2]] -> [1, 2])\n",
    "    :arg {l} - a list of list\n",
    "    :return - flattened list\n",
    "    \"\"\"\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "def _clean_text(text, stopwords=set(stopwords.words(\"english\"))): #, lemmatizer=WordNetLemmatizer()):\n",
    "    \"\"\"\n",
    "    This function is used for the preprocessing step, which will\n",
    "    - convert text to lowercase\n",
    "    - remove quotations surrounding the word (e.g. 'perks' -> perks)\n",
    "    - handle some contraction of words (e.g. he's -> he is, can't -> cannot)\n",
    "    - remove multiple consecutive spaces\n",
    "    - remove the space that starts or ends in the sentence\n",
    "    - remove stopwords\n",
    "    (Note: the lemmatization has been done for this and we found that it did not provide a better result)\n",
    "    :arg {text} - a string (sentence)\n",
    "    :arg {stopwords} - a set of words \n",
    "    :return - preprocessed string\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\'(\\w*)\\'\", r\"\\1\", text)\n",
    "    text = re.sub(r\"(he|she|it)\\'s\", r\"\\1 is\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    text = \" \".join([w for w in word_tokenize(text) if not w in stopwords])\n",
    "    return text\n",
    "\n",
    "def _readXML(filename):\n",
    "    \"\"\"\n",
    "    This function is to read SemEval Dataset in XML format. Here, we only 7 columns, which are:\n",
    "    ['review', 'term', 'termPolarity', 'startIndex', 'endIndex','aspect', 'aspectPolarity']\n",
    "    :arg {filename} - the dataset file (e.g. \"Restaurant_Train.xml\")\n",
    "    :return - pandas dataframe\n",
    "    \"\"\"\n",
    "    table = []\n",
    "    row = [np.NaN] * 7\n",
    "    \n",
    "    for event, node in et.iterparse(filename, events=(\"start\", \"end\")):\n",
    "\n",
    "        if node.tag == \"text\":\n",
    "            row[0] = node.text\n",
    "        elif node.tag == \"aspectTerms\" and event == \"start\":\n",
    "            row[1] = []\n",
    "            row[2] = []\n",
    "            row[3] = []\n",
    "            row[4] = []\n",
    "        elif node.tag == \"aspectTerm\" and event == \"start\":\n",
    "            row[1].append(node.attrib.get(\"term\").replace(\"-\", \" \").replace(\"/\", \" \"))\n",
    "            row[2].append(node.attrib.get(\"polarity\"))\n",
    "            row[3].append(int(node.attrib.get(\"from\")))\n",
    "            row[4].append(int(node.attrib.get(\"to\")))\n",
    "        elif node.tag == \"aspectCategories\" and event == \"start\":\n",
    "            row[5] = []\n",
    "            row[6] = []\n",
    "        elif node.tag == \"aspectCategory\" and event == \"start\":\n",
    "            row[5].append(node.attrib.get(\"category\"))\n",
    "            row[6].append(node.attrib.get(\"polarity\"))\n",
    "        elif node.tag == \"aspectCategories\" and event == \"end\":\n",
    "            table.append(row)\n",
    "            row = [np.NaN] * 7\n",
    "\n",
    "    dfcols = ['review', 'term', 'termPolarity', 'startIndex', 'endIndex','aspect', 'aspectPolarity']\n",
    "    data = pd.DataFrame(table, columns=dfcols)\n",
    "    data[\"review\"] = data[\"review\"].str.replace(\"-\", \" \")\n",
    "    data[\"review\"] = data[\"review\"].str.replace(\"/\", \" \")\n",
    "    return data\n",
    "    \n",
    "def _add6PosFeautures(sentences, max_sent_len = 65):\n",
    "    \"\"\"\n",
    "    This function is specially made for add 6 POS tag features for the model we have trained.\n",
    "    :arg {sentences} - list of sentences\n",
    "    :arg {max_sent_len} - the maximum sentence length (by default it would be 65)\n",
    "    :return - pos features for given list of sentences\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    pos_tags = [\"CC\",\"NN\",\"JJ\",\"VB\",\"RB\",\"IN\"]\n",
    "    le.fit(pos_tags)\n",
    "    input_data = np.zeros((len(sentences), max_sent_len, len(pos_tags)))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = text_to_word_sequence(sentence)\n",
    "        tags = nltk.pos_tag(words)\n",
    "        sentence_len = len(tags)\n",
    "        \n",
    "        for j in range(max_sent_len):\n",
    "            if j< sentence_len :\n",
    "                curr_tag = tags[j][1][:2] # only see the first two letters\n",
    "                if curr_tag in pos_tags:                    \n",
    "                    index = (le.transform([curr_tag]))[0]\n",
    "                    input_data[i][j][index] = 1\n",
    "\n",
    "    return np.asarray(input_data)\n",
    "\n",
    "def _oneHotVectorize(df, mlb, le):\n",
    "    \"\"\"\n",
    "    This function acts as a vectorizer that turns a list of aspects into one-hot vector.\n",
    "    However, it is modified to accommodate a multilabel pattern.\n",
    "    :arg {df} - a dataframe (in this case, it would be our dataset)\n",
    "    :arg {mlb} - a multilabel binarizer (from module \"sklearn\")\n",
    "    :arg {le} - a label encoder (from module \"sklearn\")\n",
    "    :return - processed dataframe\n",
    "    \"\"\"\n",
    "    df = df.apply(le.transform)\n",
    "    df = mlb.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "def _modified_performance_measure(preds, label=None):\n",
    "    \"\"\"\n",
    "    This function is a modified version of _performance_measure.\n",
    "    The original _performance_measure function needs a model as its first input.\n",
    "    And the prediction by the model is unpredictable as it would be in a form of a list of 5 probabilities.\n",
    "    Therefore, we modify the function by giving the prediction here and test whether the rest of the code is correct. \n",
    "    :arg {preds} - prediction in probabilities\n",
    "    :arg {label} - the label for the input \n",
    "    :return - accuracy, precision, recall and f1 in a list\n",
    "    \"\"\"\n",
    "    processed_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        pred = list(map(lambda val: 1 if val > 0.175 else 0, preds[i]))\n",
    "        processed_preds.append(pred)\n",
    "        \n",
    "    # return the prediction if no label is provided.\n",
    "    # as this would be in the case where users just want to see the output of model given their inputs \n",
    "    if label is None:\n",
    "        return processed_preds\n",
    "\n",
    "    test_label = processed_preds\n",
    "    true_label = label\n",
    "\n",
    "    total_pos = .0\n",
    "    total_neg = .0\n",
    "    tp = .0 # True Positive\n",
    "    tn = .0 # True Negative\n",
    "    for i in range(len(test_label)):\n",
    "        for j in range(len(test_label[0])):\n",
    "            if test_label[i][j] == 1:\n",
    "                total_pos += 1\n",
    "                if true_label[i][j] ==1:\n",
    "                    tp +=1\n",
    "            if test_label[i][j] == 0:\n",
    "                total_neg += 1\n",
    "                if true_label[i][j] ==0:\n",
    "                    tn += 1\n",
    "    fp = total_neg - tn # False Positive\n",
    "    fn = total_pos - tp # False Negative\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/total_pos\n",
    "    f1 = 2 * (precision * recall)/(precision + recall)\n",
    "    acc = (tp + tn)/(total_pos + total_neg)\n",
    "\n",
    "    return list(map(lambda x:round(x,2),[acc, precision, recall, f1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x10d2f1d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def test_flatten(self):\n",
    "        \"\"\"\n",
    "        Test function _flatten\n",
    "        \"\"\"\n",
    "        self.assertEqual(_flatten([[1], [2]]), [1, 2])\n",
    "        self.assertEqual(_flatten([[\"a\",\"b\"], [\"c\"]]), [\"a\", \"b\", \"c\"])\n",
    "        self.assertFalse(_flatten([[[1]], [[2]]]) == [[1], 2]) # Output should be [[1], [2]]\n",
    "        self.assertTrue(_flatten([[1], [\"a\"], [2], [\"b\"]]), [1, \"a\", 2, \"b\"])\n",
    "        self.assertTrue(_flatten([[]]) == [])\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.033s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1a40153208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def test_clean_text(self):\n",
    "        \"\"\"\n",
    "        Test function _clean_text\n",
    "        \"\"\"\n",
    "        # test for lowercase\n",
    "        self.assertEqual(_clean_text(\"ABC\"), \"abc\")\n",
    "        # test for stopword removal\n",
    "        self.assertEqual(_clean_text(\"the food is nice.\"), \"food nice .\")\n",
    "        # test for removing quotations that surround words\n",
    "        self.assertEqual(_clean_text(\"'food' 'is' 'nice'\"), \"food nice\")\n",
    "        # test for contraction he's, she's and it's (if not expanded, then \"'s\" will not be treated as a stopword)\n",
    "        self.assertEqual(_clean_text(\"He's a good boy and she's a good girl. It's not a good dog.\"), \\\n",
    "                                     \"good boy good girl . good dog .\")\n",
    "        # test for contraction can't, 'll, n't (if not expanded, then they might not be treated as a stopword)\n",
    "        self.assertEqual(_clean_text(\"I don't think I can't win the game but I'll lose him if you didn't ask me.\"), \\\n",
    "                                     \"think win game lose ask .\")\n",
    "        # test for contraction i'm and 're (if not expanded, then \"'s\" will not be treated as a stopword)\n",
    "        self.assertEqual(_clean_text(\"I'm not going to that place but you're going to that place.\"), \\\n",
    "                                     \"going place going place .\")\n",
    "        # test for multiple consecutive whitespace removal\n",
    "        self.assertEqual(_clean_text(\"dim     sum     is     good\"), \\\n",
    "                                     \"dim sum good\")\n",
    "        # test for starting and ending whitespace removal\n",
    "        self.assertEqual(_clean_text(\" dim sum is good \"), \\\n",
    "                                     \"dim sum good\")\n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.032s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1a46bb5c18>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This function is to create some examples used for testing\n",
    "        \"\"\"\n",
    "        self.test_xml_file1 = \"./Datasets/test_readXML_1.xml\"\n",
    "        self.test_xml_file2 = \"./Datasets/test_readXML_2.xml\"\n",
    "\n",
    "    def test_readXML(self):\n",
    "        \"\"\"\n",
    "        test function _readXML\n",
    "        \"\"\"\n",
    "        # Dataframes created from our function\n",
    "        df1 = _readXML(self.test_xml_file1)\n",
    "        df2 = _readXML(self.test_xml_file2)\n",
    "\n",
    "        # Manually created dataframes\n",
    "        check_df1 = pd.DataFrame(columns=['review', 'term', 'termPolarity', 'startIndex', 'endIndex','aspect', 'aspectPolarity'])\n",
    "        check_df1 = check_df1.append(pd.Series(['The food is nice!', ['food'], \\\n",
    "                                    ['positive'], [4], [8], ['food'], ['positive']], \\\n",
    "                                   index=check_df1.columns ), ignore_index=True)\n",
    "        check_df1 = check_df1.append(pd.Series(['It would be better if there are some peppers on it.', ['peppers'], \\\n",
    "                                    ['neutral'], [37], [44], ['food'], ['neutral']], \\\n",
    "                                   index=check_df1.columns ), ignore_index=True)\n",
    "        \n",
    "        check_df2 = pd.DataFrame(columns=['review', 'term', 'termPolarity', 'startIndex', 'endIndex','aspect', 'aspectPolarity'])\n",
    "        check_df2 = check_df2.append(pd.Series(['I hate that waiter.', ['waiter'], \\\n",
    "                                    ['negative'], [12], [18], ['service'], ['negative']], \\\n",
    "                                   index=check_df2.columns ), ignore_index=True)\n",
    "        check_df2 = check_df2.append(pd.Series(['The fish and chips is so delicious.', ['fish and chips'], \\\n",
    "                                    ['positive'], [4], [18], ['food'], ['positive']], \\\n",
    "                                   index=check_df2.columns ), ignore_index=True)        \n",
    "\n",
    "        # Check whether they are equal\n",
    "        self.assertTrue(df1.equals(check_df1))\n",
    "        self.assertTrue(df2.equals(check_df2))        \n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.017s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1a4900f978>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This function is to create some examples used for testing\n",
    "        \"\"\"\n",
    "        self.sentences1 = [\n",
    "            \"The food is nice!\",\n",
    "            \"It would be better if there are some peppers on it.\"\n",
    "        ]\n",
    "        self.sentences2 = [\n",
    "            \"I hate that waiter.\",\n",
    "            \"The fish and chips is so delicious.\"\n",
    "        ]\n",
    "\n",
    "    def test_add6PosFeatures(self):\n",
    "        \"\"\"\n",
    "        test function _add6PosFeatures\n",
    "        \"\"\" \n",
    "        # Part-of-Speech sequence = [\"CC\",\"IN\",\"JJ\",\"NN\",\"RB\",\"VB\"]\n",
    "        # Max Sentence Length = 65\n",
    "        # The Part-of-Speech features provided by our function\n",
    "        pos_features1 = _add6PosFeautures(self.sentences1)\n",
    "        pos_features2 = _add6PosFeautures(self.sentences2)\n",
    "        \n",
    "        # Manually created Part-of-Speech features\n",
    "        features1 = np.zeros((len(self.sentences1), 65, 6))\n",
    "        for i, sentence in enumerate(self.sentences1):\n",
    "            words1 = text_to_word_sequence(sentence)\n",
    "            pos1 = nltk.pos_tag(words1)\n",
    "            for j, pos in enumerate(pos1):\n",
    "                if pos[1][:2] == \"CC\":\n",
    "                    features1[i][j][0] = 1\n",
    "                elif pos[1][:2] == \"NN\":\n",
    "                    features1[i][j][3] = 1\n",
    "                elif pos[1][:2] == \"JJ\":\n",
    "                    features1[i][j][2] = 1\n",
    "                elif pos[1][:2] == \"VB\":\n",
    "                    features1[i][j][5] = 1\n",
    "                elif pos[1][:2] == \"RB\":\n",
    "                    features1[i][j][4] = 1\n",
    "                elif pos[1][:2] == \"IN\":\n",
    "                    features1[i][j][1] = 1\n",
    "                    \n",
    "        features2 = np.zeros((len(self.sentences2), 65, 6))\n",
    "        for i, sentence in enumerate(self.sentences2):\n",
    "            words2 = text_to_word_sequence(sentence)\n",
    "            pos2 = nltk.pos_tag(words2)\n",
    "            for j, pos in enumerate(pos2):\n",
    "                if pos[1][:2] == \"CC\":\n",
    "                    features2[i][j][0] = 1\n",
    "                elif pos[1][:2] == \"NN\":\n",
    "                    features2[i][j][3] = 1\n",
    "                elif pos[1][:2] == \"JJ\":\n",
    "                    features2[i][j][2] = 1\n",
    "                elif pos[1][:2] == \"VB\":\n",
    "                    features2[i][j][5] = 1\n",
    "                elif pos[1][:2] == \"RB\":\n",
    "                    features2[i][j][4] = 1\n",
    "                elif pos[1][:2] == \"IN\":\n",
    "                    features2[i][j][1] = 1\n",
    "        \n",
    "        # Check whether they are equal\n",
    "        self.assertTrue(np.array_equal(pos_features1, features1))\n",
    "        self.assertTrue(np.array_equal(pos_features2, features2))\n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1a42aa6a58>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This function is to create some examples used for testing\n",
    "        \"\"\"\n",
    "        self.unique_asp = [\"service\",\"food\",\"price\",\"ambience\",\"anecdotes/miscellaneous\"]\n",
    "        self.mlb = MultiLabelBinarizer(classes=[i for i in range(5)])\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(self.unique_asp)\n",
    "\n",
    "    def test_oneHotVectorize(self):\n",
    "        \"\"\"\n",
    "        test function _oneHotVectorize\n",
    "        \"\"\"\n",
    "        df1 = pd.DataFrame([\n",
    "            [[\"service\"]],\n",
    "            [[\"service\",\"price\"]]\n",
    "          ], columns=[\"aspect\"])\n",
    "        \n",
    "        df2 = pd.DataFrame([\n",
    "            [[\"food\"]],\n",
    "            [[\"food\",\"service\",\"price\"]]\n",
    "          ], columns=[\"aspect\"])\n",
    "        \n",
    "        # encoded labels by our function\n",
    "        labels1 = _oneHotVectorize(df1[\"aspect\"], self.mlb, self.le)\n",
    "        labels2 = _oneHotVectorize(df2[\"aspect\"], self.mlb, self.le)        \n",
    "        \n",
    "        # manually encoded labels  \n",
    "        check_df1 = []\n",
    "        check_df2 = []\n",
    "        for i in range(2):\n",
    "            l = []\n",
    "            l2 = []\n",
    "            for j in range(5):\n",
    "                l.append(0)\n",
    "                l2.append(0)\n",
    "            check_df1.append(l)\n",
    "            check_df2.append(l2)\n",
    "            \n",
    "        for i,row in enumerate(df1[\"aspect\"]):\n",
    "            for j,aspect in enumerate(row):\n",
    "                if aspect == \"service\":\n",
    "                    check_df1[i][4]=1\n",
    "                elif aspect == \"food\":\n",
    "                    check_df1[i][2]=1\n",
    "                elif aspect == \"price\":\n",
    "                    check_df1[i][3]=1\n",
    "                elif aspect == \"ambience\":\n",
    "                    check_df1[i][0]=1\n",
    "                else:\n",
    "                    check_df1[i][1]=1\n",
    "                    \n",
    "        for i,row in enumerate(df2[\"aspect\"]):\n",
    "            for j,aspect in enumerate(row):\n",
    "                if aspect == \"service\":\n",
    "                    check_df2[i][4]=1\n",
    "                elif aspect == \"food\":\n",
    "                    check_df2[i][2]=1\n",
    "                elif aspect == \"price\":\n",
    "                    check_df2[i][3]=1\n",
    "                elif aspect == \"ambience\":\n",
    "                    check_df2[i][0]=1\n",
    "                else:\n",
    "                    check_df2[i][1]=1\n",
    "        check_df1 = np.asarray(check_df1)\n",
    "        check_df2 = np.asarray(check_df2)\n",
    "        \n",
    "        # check whether they are equal\n",
    "        self.assertTrue(np.array_equal(labels1, check_df1))\n",
    "        self.assertTrue(np.array_equal(labels2, check_df2))\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1a49c840b8>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This function is to create some examples used for testing\n",
    "        \"\"\"\n",
    "        self.predicted1 = [\n",
    "            [0.0,0.0,0.45,0.45,0.1],\n",
    "            [0.0,0.0,0.9,0.05,0.05],\n",
    "            [1.0,0.0,0.0,0.0,0.0],\n",
    "            [0.0,0.0,0.0,0.9,0.1]\n",
    "        ]\n",
    "\n",
    "        self.actual1 = [\n",
    "            [0,0,1,0,1],\n",
    "            [0,0,1,0,0],\n",
    "            [0,0,0,1,0],\n",
    "            [1,1,1,1,0]\n",
    "        ]\n",
    "        \n",
    "        self.predicted2 = [\n",
    "            [1.0,0.0,0.0,0.0,0.0],\n",
    "            [0.0,0.0,0.0,1.0,0.0]\n",
    "        ]\n",
    "\n",
    "        self.actual2 = [\n",
    "            [0,0,0,1,0],\n",
    "            [1,1,1,1,0]\n",
    "        ]\n",
    "\n",
    "    def test_modified_performance_measure(self):\n",
    "        # result from our function\n",
    "        result1 = _modified_performance_measure(self.predicted1, self.actual1)\n",
    "        result2 = _modified_performance_measure(self.predicted2, self.actual2)\n",
    "        \n",
    "        # real result\n",
    "        real_result1 =[0.65, 0.38, 0.6, 0.46]\n",
    "        real_result2 = [0.50, 0.20, 0.50, 0.29]\n",
    "        \n",
    "        # check whether they are equal\n",
    "        self.assertTrue(result1 == real_result1)\n",
    "        self.assertTrue(result2 == real_result2)\n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
