{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Testing - Unittest\n",
    "## Load python unittest module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Extraction\n",
    "### Functions that need to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Aggregation\n",
    "### Functions that need to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 13 02:23:50 2019\n",
    "\n",
    "@author: Zhiwei and Friend(s)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "###########################################\n",
    "  _      _ _                    _          \n",
    " | |    (_| |                  (_)         \n",
    " | |     _| |__  _ __ __ _ _ __ _  ___ ___ \n",
    " | |    | | '_ \\| '__/ _` | '__| |/ _ / __|\n",
    " | |____| | |_) | | | (_| | |  | |  __\\__ \\\n",
    " |______|_|_.__/|_|  \\__,_|_|  |_|\\___|___/\n",
    "                                           \n",
    "###########################################\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\"\"\"\n",
    "###############################################\n",
    "  ______                _   _                 \n",
    " |  ____|              | | (_)                \n",
    " | |__ _   _ _ __   ___| |_ _  ___  _ __  ___ \n",
    " |  __| | | | '_ \\ / __| __| |/ _ \\| '_ \\/ __|\n",
    " | |  | |_| | | | | (__| |_| | (_) | | | \\__ \\\n",
    " |_|   \\__,_|_| |_|\\___|\\__|_|\\___/|_| |_|___/\n",
    "\n",
    "###############################################\n",
    "\"\"\"\n",
    "\n",
    "# Flatten list of list\n",
    "def _flatten(l):\n",
    "    \"\"\"\n",
    "    This function will flatten a list of list to a list. (e.g. [[1],[2]] -> [1, 2])\n",
    "    :arg {l} - a list of list\n",
    "    :return - flattened list\n",
    "    \"\"\"\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "def _clean_text(text, stopwords=set(stopwords.words(\"english\"))): #, lemmatizer=WordNetLemmatizer()):\n",
    "    \"\"\"\n",
    "    This function is used for the preprocessing step, which will\n",
    "    - convert text to lowercase\n",
    "    - remove quotations surrounding the word (e.g. 'perks' -> perks)\n",
    "    - handle some contraction of words (e.g. he's -> he is, can't -> cannot)\n",
    "    - remove multiple consecutive spaces\n",
    "    - remove the space that starts or ends in the sentence\n",
    "    - remove stopwords\n",
    "    (Note: the lemmatization has been done for this and we found that it did not provide a better result)\n",
    "    :arg {text} - a string (sentence)\n",
    "    :arg {stopwords} - a set of words \n",
    "    :return - preprocessed string\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\'(\\w*)\\'\", r\"\\1\", text)\n",
    "    text = re.sub(r\"(he|she|it)\\'s\", r\"\\1 is\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    text = \" \".join([w for w in word_tokenize(text) if not w in stopwords])\n",
    "    return text\n",
    "\n",
    "def _readXML(filename):\n",
    "    \"\"\"\n",
    "    This function is to read SemEval Dataset in XML format. Here, we only 7 columns, which are:\n",
    "    ['review', 'term', 'termPolarity', 'startIndex', 'endIndex','aspect', 'aspectPolarity']\n",
    "    :arg {filename} - the dataset file (e.g. \"Restaurant_Train.xml\")\n",
    "    :return - pandas dataframe\n",
    "    \"\"\"\n",
    "    table = []\n",
    "    row = [np.NaN] * 7\n",
    "    \n",
    "    for event, node in et.iterparse(filename, events=(\"start\", \"end\")):\n",
    "\n",
    "        if node.tag == \"text\":\n",
    "            row[0] = node.text\n",
    "        elif node.tag == \"aspectTerms\" and event == \"start\":\n",
    "            row[1] = []\n",
    "            row[2] = []\n",
    "            row[3] = []\n",
    "            row[4] = []\n",
    "        elif node.tag == \"aspectTerm\" and event == \"start\":\n",
    "            row[1].append(node.attrib.get(\"term\").replace(\"-\", \" \").replace(\"/\", \" \"))\n",
    "            row[2].append(node.attrib.get(\"polarity\"))\n",
    "            row[3].append(int(node.attrib.get(\"from\")))\n",
    "            row[4].append(int(node.attrib.get(\"to\")))\n",
    "        elif node.tag == \"aspectCategories\" and event == \"start\":\n",
    "            row[5] = []\n",
    "            row[6] = []\n",
    "        elif node.tag == \"aspectCategory\" and event == \"start\":\n",
    "            row[5].append(node.attrib.get(\"category\"))\n",
    "            row[6].append(node.attrib.get(\"polarity\"))\n",
    "        elif node.tag == \"aspectCategories\" and event == \"end\":\n",
    "            table.append(row)\n",
    "            row = [np.NaN] * 7\n",
    "\n",
    "    dfcols = ['review', 'term', 'termPolarity', 'startIndex', 'endIndex','aspect', 'aspectPolarity']\n",
    "    data = pd.DataFrame(table, columns=dfcols)\n",
    "    data[\"review\"] = data[\"review\"].str.replace(\"-\", \" \")\n",
    "    data[\"review\"] = data[\"review\"].str.replace(\"/\", \" \")\n",
    "    return data\n",
    "    \n",
    "def _add6PosFeautures(sentences, max_sent_len = 65):\n",
    "    \"\"\"\n",
    "    This function is specially made for add 6 POS tag features for the model we have trained.\n",
    "    :arg {sentences} - list of sentences\n",
    "    :arg {max_sent_len} - the maximum sentence length (by default it would be 65)\n",
    "    :return - pos features for given list of sentences\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    pos_tags = [\"CC\",\"NN\",\"JJ\",\"VB\",\"RB\",\"IN\"]\n",
    "    le.fit(pos_tags)\n",
    "    input_data = np.zeros((len(sentences), max_sent_len, len(pos_tags)))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = text_to_word_sequence(sentence)\n",
    "        tags = nltk.pos_tag(words)\n",
    "        sentence_len = len(tags)\n",
    "        \n",
    "        for j in range(max_sent_len):\n",
    "            if j< sentence_len :\n",
    "                curr_tag = tags[j][1][:2] # only see the first two letters\n",
    "                if curr_tag in pos_tags:                    \n",
    "                    index = (le.transform([curr_tag]))[0]\n",
    "                    input_data[i][j][index] = 1\n",
    "\n",
    "    return np.asarray(input_data)\n",
    "\n",
    "def _oneHotVectorize(df, asp_list, mlb, le):\n",
    "    \"\"\"\n",
    "    This function acts as a vectorizer that turns a list of aspects into one-hot vector.\n",
    "    However, it is modified to accommodate a multilabel pattern.\n",
    "    :arg {df} - a dataframe (in this case, it would be our dataset)\n",
    "    :arg {asp_list} - a unique aspect list ([\"service\", \"food\", \"price\", \"ambience\", \"anecdotes/miscellaneous\"])\n",
    "    :arg {mlb} - a multilabel binarizer (from module \"sklearn\")\n",
    "    :arg {le} - a label encoder (from module \"sklearn\")\n",
    "    :return - processed dataframe\n",
    "    \"\"\"\n",
    "    df = df.apply(le.transform)\n",
    "    df = mlb.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "def _modified_performance_measure(preds, data, label=None):\n",
    "    \"\"\"\n",
    "    This function is a modified version of _performance_measure.\n",
    "    The original _performance_measure function needs a model as its first input.\n",
    "    And the prediction by the model is unpredictable as it would be in a form of a list of 5 probabilities.\n",
    "    Therefore, we modify the function by giving the prediction here and test whether the rest of the code is correct. \n",
    "    :arg {preds} - prediction in probabilities\n",
    "    :arg {data} - the input for model\n",
    "    :arg {label} - the label for the input \n",
    "    :return - None\n",
    "    \"\"\"\n",
    "    processed_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        pred = list(map(lambda val: 1 if val > 0.175 else 0, preds[i]))\n",
    "        processed_preds.append(pred)\n",
    "        \n",
    "    # return the prediction if no label is provided.\n",
    "    # as this would be in the case where users just want to see the output of model given their inputs \n",
    "    if label is None:\n",
    "        return processed_preds\n",
    "\n",
    "    test_label = processed_preds\n",
    "    true_label = label\n",
    "\n",
    "    total_pos = .0\n",
    "    total_neg = .0\n",
    "    tp = .0 # True Positive\n",
    "    tn = .0 # True Negative\n",
    "    for i in range(len(test_label)):\n",
    "        for j in range(len(test_label[0])):\n",
    "            if test_label[i][j] == 1:\n",
    "                total_pos += 1\n",
    "                if true_label[i][j] ==1:\n",
    "                    tp +=1\n",
    "            if test_label[i][j] == 0:\n",
    "                total_neg += 1\n",
    "                if true_label[i][j] ==0:\n",
    "                    tn += 1\n",
    "\n",
    "    fp = total_neg - tn # False Positive\n",
    "    fn = total_pos - tp # False Negative\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/total_pos\n",
    "    f1 = 2 * (precision * recall)/(precision + recall)\n",
    "    acc = (tp + tn)/(total_pos + total_neg)\n",
    "\n",
    "    print(\"Precision: \" + str(round(precision, 4)))\n",
    "    print(\"Recall: \" + str(round(recall, 4))) \n",
    "    print(\"F1: \" + str(round(f1, 4)))\n",
    "    print(\"Accuracy: \" + str(round(acc, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x10d2f1d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def test_flatten(self):\n",
    "        \"\"\"\n",
    "        Test function _flatten\n",
    "        \"\"\"\n",
    "        self.assertEqual(_flatten([[1], [2]]), [1, 2])\n",
    "        self.assertEqual(_flatten([[\"a\",\"b\"], [\"c\"]]), [\"a\", \"b\", \"c\"])\n",
    "        self.assertFalse(_flatten([[[1]], [[2]]]) == [[1], 2]) # Output should be [[1], [2]]\n",
    "        self.assertTrue(_flatten([[1], [\"a\"], [2], [\"b\"]]), [1, \"a\", 2, \"b\"])\n",
    "        self.assertTrue(_flatten([[]]) == [])\n",
    "        \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.033s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1a40153208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def test_clean_text(self):\n",
    "        \"\"\"\n",
    "        Test function _clean_text\n",
    "        \"\"\"\n",
    "        # test for lowercase\n",
    "        self.assertEqual(_clean_text(\"ABC\"), \"abc\")\n",
    "        # test for stopword removal\n",
    "        self.assertEqual(_clean_text(\"the food is nice.\"), \"food nice .\")\n",
    "        # test for removing quotations that surround words\n",
    "        self.assertEqual(_clean_text(\"'food' 'is' 'nice'\"), \"food nice\")\n",
    "        # test for contraction he's, she's and it's (if not expanded, then \"'s\" will not be treated as a stopword)\n",
    "        self.assertEqual(_clean_text(\"He's a good boy and she's a good girl. It's not a good dog.\"), \\\n",
    "                                     \"good boy good girl . good dog .\")\n",
    "        # test for contraction can't, 'll, n't (if not expanded, then they might not be treated as a stopword)\n",
    "        self.assertEqual(_clean_text(\"I don't think I can't win the game but I'll lose him if you didn't ask me.\"), \\\n",
    "                                     \"think win game lose ask .\")\n",
    "        # test for contraction i'm and 're (if not expanded, then \"'s\" will not be treated as a stopword)\n",
    "        self.assertEqual(_clean_text(\"I'm not going to that place but you're going to that place.\"), \\\n",
    "                                     \"going place going place .\")\n",
    "        # test for multiple consecutive whitespace removal\n",
    "        self.assertEqual(_clean_text(\"dim     sum     is     good\"), \\\n",
    "                                     \"dim sum good\")\n",
    "        # test for starting and ending whitespace removal\n",
    "        self.assertEqual(_clean_text(\" dim sum is good \"), \\\n",
    "                                     \"dim sum good\")\n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This function is to create some examples used for testing\n",
    "        \"\"\"\n",
    "        test_xml_file1 = \"./Datasets/test_readXML_1.xml\"\n",
    "        test_xml_file2 = \"./Datasets/test_readXML_2.xml\"\n",
    "\n",
    "    def test_readXML(self):\n",
    "        \"\"\"\n",
    "        test function _readXML\n",
    "        \"\"\"\n",
    "        df1 = _readXML(test_xml_file1)\n",
    "        df2 = _readXML(test_xml_file2)\n",
    "        check_df1 = pd.DataFrame()\n",
    "        \n",
    "        self.assertEqual(df1)\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestFunc(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"\n",
    "        This function is to create some examples used for testing\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def test_readXML(self):\n",
    "        pass\n",
    "\n",
    "    def test_add6PosFeatures(self):\n",
    "        pass\n",
    "\n",
    "    def test_oneHotVectorize(self):\n",
    "        pass\n",
    "\n",
    "    def test_performance_measure(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
